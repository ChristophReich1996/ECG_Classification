\section{Discussion} \label{sec:discussion}
As observed in the ablations (Sec. \ref{subsec:ablations}) training without data augmentation leads to a zero valued loss. Experimental results without data augmentation are presented in Table \ref{tab:ablations}. Utilizing the proposed augmentation pipeline improves generalization. Unfortionalty, the use of a sophisticated augmentation pipeline comes with additional hyperparameters. We set the hyperparameters of our augmentation pipeline empirically based on a few test training runs. To use the full potential of the proposed augmentation pipeline hyperparameter optimization \cite{Goodfellow2016, Cubuk2019} or the development of an adaptive approach, such as \cite{Fawzi2016, Karras2020}, may lead to improvements.\\
\indent We also observe the phenomenon that an increase in the model size with and without data augmentation does not lead to overfitting but more generalization. We have no clear explanation for this behavior. A potential description could be the double decent phenomenon \cite{Belkin2019, Nakkiran2020}, without an overfitting bump when regularization (data augmentation \& dropout) is applied. A large-scale study with more network sizes may help to understand the observed phenomenon.\\
\indent Surprisingly, performing pre-training on the Icentia11k does not lead to better classification performance on the target dataset. This could be due to the fact that the dataset used for pre-training is too out-of-domain transferring knowledge to the target dataset.